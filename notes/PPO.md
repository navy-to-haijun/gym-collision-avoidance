# PPO(proximal policy optimization)近端策略优化

##  同策略、异策略

* 同策略： 学习的智能体和与环境交互的智能体是相同的；
* 异策略： 学习的智能体和与环境交互的智能体不是相同的；

策略梯度： 需要一个智能体、一个策略和一个演员。演员去与环境交互搜集数据，搜集很多的轨迹 $\tau$，根据搜集到的数据按照策略梯度的公式更新策略的参数。所有策略梯度是同策略。PO是策略梯度的变形

策略梯度是一个会花很多时间来采样数据的算法，其大多数时间都在采样数据。为了节约时间，我们想要从同策略变成异策略：



## PPO - RNN 

实例化 PPO_discrete_RNN()

## RNN pytorch



